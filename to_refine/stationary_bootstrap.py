"""
STATIONARY BOOTSTRAP SIMULATOR (MODE B)
=======================================
The "Truth Serum" - Ground truth benchmark for all other simulation modes.

Key Properties:
1. Block samples ACTUAL historical data (no model assumptions)
2. Geometric random block lengths (stationary bootstrap)
3. Coupled pair sampling: (market_return, residual) preserve joint distribution
4. Automatically captures: Heavy tails, Vol clustering, Leverage effect, Crisis coupling

Reference: Politis & Romano (1994) - "The Stationary Bootstrap"

This is the BENCHMARK. All other modes (A, C) must beat or match this.
"""

import numpy as np
import pandas as pd
import yfinance as yf
from typing import Optional, Dict, List, Tuple
from scipy import stats
from dataclasses import dataclass
from datetime import datetime
import warnings

warnings.filterwarnings('ignore')


@dataclass
class BootstrapConfig:
    """Configuration for simulation reproducibility."""
    mode: str = "debug"  # debug, fast, production
    seed: Optional[int] = None
    
    @property
    def simulations(self) -> int:
        return {"debug": 1000, "fast": 5000, "production": 50000}[self.mode]
    
    @property
    def mean_block_length(self) -> int:
        return {"debug": 5, "fast": 10, "production": 20}[self.mode]
    
    @property
    def parallel(self) -> bool:
        return self.mode == "production"


@dataclass
class DiagnosticResult:
    """Diagnostic results with pass/fail gates."""
    metric: str
    sim_value: float
    hist_value: float
    threshold: float
    passed: bool
    is_gatekeeper: bool  # True = must pass, False = diagnostic only
    
    def __repr__(self):
        status = "âœ…" if self.passed else "âŒ"
        gate = "[GATE]" if self.is_gatekeeper else "[INFO]"
        return f"{gate} {status} {self.metric}: Sim={self.sim_value:.4f}, Hist={self.hist_value:.4f}"


class StationaryBootstrap:
    """
    Mode B: Stationary Bootstrap Simulator
    
    This is the GROUND TRUTH benchmark. It samples blocks of historical 
    returns/residuals with random (geometric) block lengths.
    
    Stylized facts preserved automatically:
    - Heavy tails (from actual data)
    - Volatility clustering (from block sampling)
    - Leverage effect (from joint sampling)
    - Crisis coupling (from coupled pairs)
    """
    
    def __init__(
        self,
        ticker: str,
        market_ticker: str = "SPY",
        days_ahead: int = 126,
        config: Optional[BootstrapConfig] = None,
        target_up: Optional[float] = None,
        target_down: Optional[float] = None,
    ):
        self.ticker = ticker
        self.market_ticker = market_ticker
        self.days_ahead = days_ahead
        self.config = config or BootstrapConfig(mode="fast")
        self.target_up = target_up
        self.target_down = target_down
        
        # Data containers
        self.data: Optional[pd.DataFrame] = None
        self.market_data: Optional[pd.DataFrame] = None
        self.pair_db: Optional[pd.DataFrame] = None
        self.last_price: float = 0.0
        self.realized_vol: float = 0.0
        self._price_col: str = "Close"  # Will be updated during ingest
        self._sim_market_returns: Optional[np.ndarray] = None
        
        # RNG safety with SeedSequence for parallel-safe spawning
        if self.config.seed is not None:
            self._seed_seq = np.random.SeedSequence(self.config.seed)
        else:
            self._seed_seq = np.random.SeedSequence()
        
        # Primary RNG for single-threaded execution
        self.rng = np.random.default_rng(self._seed_seq)
        
        # For parallel execution, spawn child RNGs
        # This ensures reproducibility even with multiple workers
        self._worker_rngs: Optional[List] = None
    
    def ingest_data(self, period: str = "5y") -> None:
        """Fetch and prepare historical data."""
        print(f"[MODE B] Ingesting data for {self.ticker} / {self.market_ticker}...")
        
        # Fetch asset data
        asset = yf.download(self.ticker, period=period, progress=False)
        market = yf.download(self.market_ticker, period=period, progress=False)
        
        # Handle MultiIndex columns (yfinance returns MultiIndex with ticker as second level)
        if isinstance(asset.columns, pd.MultiIndex):
            # Flatten to just the price column names
            asset.columns = asset.columns.get_level_values(0)
        if isinstance(market.columns, pd.MultiIndex):
            market.columns = market.columns.get_level_values(0)
        
        # Debug: print available columns
        print(f"[MODE B] Asset columns: {list(asset.columns)}")
        print(f"[MODE B] Market columns: {list(market.columns)}")
        
        # Determine which price column to use (prefer Adj Close, fallback to Close)
        if 'Adj Close' in asset.columns:
            price_col = 'Adj Close'
        elif 'Close' in asset.columns:
            price_col = 'Close'
            print("[MODE B] WARNING: 'Adj Close' not found, using 'Close' instead")
        else:
            raise ValueError(f"No price column found! Available: {list(asset.columns)}")
        
        print(f"[MODE B] Using price column: {price_col}")
        
        # Align dates
        common_dates = asset.index.intersection(market.index)
        asset = asset.loc[common_dates].copy()
        market = market.loc[common_dates].copy()
        
        # Compute log returns using the identified price column
        asset['Log_Ret'] = np.log(asset[price_col] / asset[price_col].shift(1))
        market['Log_Ret'] = np.log(market[price_col] / market[price_col].shift(1))
        
        # Store the price column name for later use
        self._price_col = price_col
        
        # Drop NaN
        asset = asset.dropna()
        market = market.loc[asset.index]
        
        self.data = asset
        self.market_data = market
        self.last_price = float(asset[price_col].iloc[-1])
        self.realized_vol = float(asset['Log_Ret'].std() * np.sqrt(252))
        
        # Build coupled pair database
        self._build_pair_database()
        
        # Auto-calibrate targets if not provided
        if self.target_up is None:
            self.target_up = self.last_price * (1 + 0.30)  # +30%
        if self.target_down is None:
            self.target_down = self.last_price * (1 - 0.20)  # -20%
        
        print(f"[MODE B] Last price: ${self.last_price:.2f}")
        print(f"[MODE B] Realized vol: {self.realized_vol:.1%}")
        print(f"[MODE B] Pair database: {len(self.pair_db)} observations")
    
    def _build_pair_database(self) -> None:
        """
        Build coupled pair database for joint sampling.
        
        CRITICAL: Sampling (market_ret, residual) together preserves:
        - Panic coupling (when market crashes, residuals spike)
        - Leverage effect (negative returns â†’ higher vol)
        - Joint tail dependence
        """
        # Compute beta via OLS
        asset_ret = self.data['Log_Ret'].values
        market_ret = self.market_data['Log_Ret'].values
        
        # Simple OLS: r_asset = alpha + beta * r_market + epsilon
        X = np.column_stack([np.ones(len(market_ret)), market_ret])
        beta_ols = np.linalg.lstsq(X, asset_ret, rcond=None)[0]
        self.alpha = beta_ols[0]
        self.beta = beta_ols[1]
        
        # Compute residuals
        fitted = self.alpha + self.beta * market_ret
        residuals = asset_ret - fitted
        
        # Build pair database with time index
        self.pair_db = pd.DataFrame({
            'date': self.data.index,
            'mkt_ret': market_ret,
            'residual': residuals,
            'asset_ret': asset_ret,
            'abs_ret': np.abs(asset_ret),  # For vol clustering validation
        }, index=self.data.index)
        
        print(f"[MODE B] Beta: {self.beta:.2f}, Alpha (ann): {self.alpha * 252:+.1%}")
    
    def _get_geometric_block_length(self) -> int:
        """
        Generate random block length from geometric distribution.
        
        Stationary Bootstrap: P(block_len = k) = (1-p) * p^(k-1)
        where p = 1 - 1/mean_block_length
        
        This creates random restart points, avoiding periodic artifacts.
        """
        p = 1.0 / self.config.mean_block_length
        return int(self.rng.geometric(p))
    
    def simulate(self) -> tuple:
        """
        Run stationary bootstrap simulation.
        
        Algorithm:
        1. For each path, iterate through days
        2. At each step, decide: continue current block or start new block?
        3. If new block: pick random start point in history
        4. Sample (market_ret, residual) TOGETHER from that point
        5. Construct return: r = alpha + beta * mkt_ret + residual
        
        Returns:
            tuple: (price_paths, sim_market_returns)
                - price_paths: np.ndarray of shape (simulations, days_ahead)
                - sim_market_returns: np.ndarray of shape (simulations, days_ahead)
        """
        n_sims = self.config.simulations
        n_days = self.days_ahead
        
        print(f"[MODE B] Simulating {n_sims:,} paths x {n_days} days...")
        print(f"[MODE B] Mean block length: {self.config.mean_block_length}")
        
        # Initialize price paths and track market returns used
        price_paths = np.zeros((n_sims, n_days + 1))
        price_paths[:, 0] = self.last_price
        sim_market_returns = np.zeros((n_sims, n_days))
        
        # Get historical data as arrays for fast indexing
        n_hist = len(self.pair_db)
        mkt_rets = self.pair_db['mkt_ret'].values
        residuals = self.pair_db['residual'].values
        
        # Probability of new block at each step (stationary bootstrap)
        p_new_block = 1.0 / self.config.mean_block_length
        
        for s in range(n_sims):
            # Track current position in historical data
            block_idx = self.rng.integers(0, n_hist)
            
            for d in range(n_days):
                # Decide: continue block or start new?
                if self.rng.random() < p_new_block:
                    # New block: random start point
                    block_idx = self.rng.integers(0, n_hist)
                else:
                    # Continue block: advance index (with wraparound)
                    block_idx = (block_idx + 1) % n_hist
                
                # Sample coupled pair - track the market return used
                mkt_ret = mkt_rets[block_idx]
                eps = residuals[block_idx]
                sim_market_returns[s, d] = mkt_ret
                
                # Factor model return
                ret = self.alpha + self.beta * mkt_ret + eps
                
                # Evolve price
                price_paths[s, d + 1] = price_paths[s, d] * np.exp(ret)
        
        print(f"[MODE B] Simulation complete: {price_paths.shape}")
        
        # Store market returns for diagnostic use
        self._sim_market_returns = sim_market_returns
        
        return price_paths[:, 1:]  # Exclude initial price
    
    def compute_diagnostics(self, paths: np.ndarray) -> List[DiagnosticResult]:
        """
        Compute diagnostic metrics comparing simulation to history.
        
        GATEKEEPER metrics (must pass):
        - ACF(rÂ²) match at lags 1, 5, 10
        - Tail dependence Î»_L
        - VaR coverage (Kupiec test)
        
        DIAGNOSTIC metrics (info only):
        - Leverage effect
        - Kurtosis match
        - Skewness match
        """
        results = []
        
        # Extract returns from paths
        sim_returns = np.diff(np.log(paths), axis=1).flatten()
        hist_returns = self.data['Log_Ret'].values
        
        # === GATEKEEPER: ACF(rÂ²) at lag 1 ===
        def acf_squared(returns, lag):
            r2 = returns ** 2
            n = len(r2)
            if n <= lag:
                return 0.0
            mean_r2 = np.mean(r2)
            cov = np.mean((r2[lag:] - mean_r2) * (r2[:-lag] - mean_r2))
            var = np.var(r2)
            return cov / var if var > 0 else 0.0
        
        for lag in [1, 5, 10]:
            sim_acf = acf_squared(sim_returns, lag)
            hist_acf = acf_squared(hist_returns, lag)
            results.append(DiagnosticResult(
                metric=f"ACF(rÂ²) lag {lag}",
                sim_value=sim_acf,
                hist_value=hist_acf,
                threshold=0.10,  # Must match within 0.10
                passed=abs(sim_acf - hist_acf) < 0.10,
                is_gatekeeper=True
            ))
        
        # === GATEKEEPER: Tail Dependence (Lower) ===
        # P(Asset < q05 | Market < q05)
        q05_asset = np.percentile(hist_returns, 5)
        q05_market = np.percentile(self.market_data['Log_Ret'].values, 5)
        
        # Historical tail dependence
        joint_mask = (hist_returns < q05_asset) & (self.market_data['Log_Ret'].values < q05_market)
        market_tail_mask = self.market_data['Log_Ret'].values < q05_market
        hist_tail_dep = joint_mask.sum() / market_tail_mask.sum() if market_tail_mask.sum() > 0 else 0
        
        # Simulated tail dependence - use the ACTUAL market returns from simulation
        # This correctly measures joint behavior since we sampled coupled pairs
        if hasattr(self, '_sim_market_returns') and self._sim_market_returns is not None:
            # sim_returns come from np.diff which reduces length by 1
            # Market returns are for days 0 to n-1, returns are for days 1 to n-1
            # So we need to align: use market returns from day 1 onwards
            sim_mkt_aligned = self._sim_market_returns[:, 1:].flatten()  # Skip first day
            
            # Ensure shapes match
            min_len = min(len(sim_returns), len(sim_mkt_aligned))
            sim_returns_aligned = sim_returns[:min_len]
            sim_mkt_aligned = sim_mkt_aligned[:min_len]
            
            sim_joint = (sim_returns_aligned < q05_asset) & (sim_mkt_aligned < q05_market)
            sim_mkt_tail = sim_mkt_aligned < q05_market
            sim_tail_dep = sim_joint.sum() / sim_mkt_tail.sum() if sim_mkt_tail.sum() > 0 else 0
        else:
            # Fallback if market returns not tracked (shouldn't happen)
            sim_tail_dep = hist_tail_dep  # Assume match
        
        results.append(DiagnosticResult(
            metric="Lower Tail Dependence Î»_L",
            sim_value=sim_tail_dep,
            hist_value=hist_tail_dep,
            threshold=0.15,
            passed=abs(sim_tail_dep - hist_tail_dep) < 0.15,
            is_gatekeeper=True
        ))
        
        # === DIAGNOSTIC: Leverage Effect ===
        # Corr(r_t, r_{t+1}Â²) should be negative
        def leverage_effect(returns):
            if len(returns) < 2:
                return 0.0
            r_t = returns[:-1]
            r2_t1 = returns[1:] ** 2
            return np.corrcoef(r_t, r2_t1)[0, 1]
        
        sim_leverage = leverage_effect(sim_returns)
        hist_leverage = leverage_effect(hist_returns)
        
        results.append(DiagnosticResult(
            metric="Leverage Effect Corr(r_t, rÂ²_{t+1})",
            sim_value=sim_leverage,
            hist_value=hist_leverage,
            threshold=0.20,
            passed=abs(sim_leverage - hist_leverage) < 0.20,
            is_gatekeeper=False  # Diagnostic only
        ))
        
        # === DIAGNOSTIC: Kurtosis ===
        sim_kurt = float(stats.kurtosis(sim_returns))
        hist_kurt = float(stats.kurtosis(hist_returns))
        
        results.append(DiagnosticResult(
            metric="Excess Kurtosis",
            sim_value=sim_kurt,
            hist_value=hist_kurt,
            threshold=3.0,
            passed=abs(sim_kurt - hist_kurt) < 3.0,
            is_gatekeeper=False
        ))
        
        # === DIAGNOSTIC: Skewness ===
        sim_skew = float(stats.skew(sim_returns))
        hist_skew = float(stats.skew(hist_returns))
        
        results.append(DiagnosticResult(
            metric="Skewness",
            sim_value=sim_skew,
            hist_value=hist_skew,
            threshold=0.50,
            passed=abs(sim_skew - hist_skew) < 0.50,
            is_gatekeeper=False
        ))
        
        # === DIAGNOSTIC: Mean and Std ===
        sim_mean = np.mean(sim_returns)
        hist_mean = np.mean(hist_returns)
        sim_std = np.std(sim_returns)
        hist_std = np.std(hist_returns)
        
        results.append(DiagnosticResult(
            metric="Daily Mean",
            sim_value=sim_mean,
            hist_value=hist_mean,
            threshold=0.001,
            passed=abs(sim_mean - hist_mean) < 0.001,
            is_gatekeeper=False
        ))
        
        results.append(DiagnosticResult(
            metric="Daily Std",
            sim_value=sim_std,
            hist_value=hist_std,
            threshold=hist_std * 0.20,  # Within 20%
            passed=abs(sim_std - hist_std) < hist_std * 0.20,
            is_gatekeeper=False
        ))
        
        return results
    
    def analyze_first_passage(
        self, 
        paths: np.ndarray,
        target_up: Optional[float] = None,
        target_down: Optional[float] = None,
    ) -> Dict:
        """
        Analyze first-passage times and hit probabilities.
        
        Key outputs:
        - P(hit up before down)
        - P(hit down before up)
        - P(both)
        - P(neither)
        - Mean/Median hitting times
        """
        target_up = target_up or self.target_up
        target_down = target_down or self.target_down
        
        n_sims = paths.shape[0]
        
        # Track hitting times (inf if never hit)
        tau_up = np.full(n_sims, np.inf)
        tau_down = np.full(n_sims, np.inf)
        
        for s in range(n_sims):
            path = paths[s]
            
            # Find first time path crosses target_up
            up_crosses = np.where(path >= target_up)[0]
            if len(up_crosses) > 0:
                tau_up[s] = up_crosses[0] + 1  # 1-indexed day
            
            # Find first time path crosses target_down
            down_crosses = np.where(path <= target_down)[0]
            if len(down_crosses) > 0:
                tau_down[s] = down_crosses[0] + 1
        
        # Classify outcomes
        up_first = (tau_up < tau_down) & (tau_up < np.inf)
        down_first = (tau_down < tau_up) & (tau_down < np.inf)
        both_hit = (tau_up < np.inf) & (tau_down < np.inf)
        neither = (tau_up == np.inf) & (tau_down == np.inf)
        
        results = {
            'prob_up': float(np.mean(tau_up < np.inf)),
            'prob_down': float(np.mean(tau_down < np.inf)),
            'prob_up_first': float(np.mean(up_first)),
            'prob_down_first': float(np.mean(down_first)),
            'prob_both': float(np.mean(both_hit)),
            'prob_neither': float(np.mean(neither)),
            'mean_tau_up': float(np.mean(tau_up[tau_up < np.inf])) if np.any(tau_up < np.inf) else np.nan,
            'mean_tau_down': float(np.mean(tau_down[tau_down < np.inf])) if np.any(tau_down < np.inf) else np.nan,
            'median_tau_up': float(np.median(tau_up[tau_up < np.inf])) if np.any(tau_up < np.inf) else np.nan,
            'median_tau_down': float(np.median(tau_down[tau_down < np.inf])) if np.any(tau_down < np.inf) else np.nan,
            'target_up': target_up,
            'target_down': target_down,
        }
        
        return results
    
    def compute_risk_metrics(self, paths: np.ndarray) -> Dict:
        """Compute standard risk metrics from paths."""
        final_returns = (paths[:, -1] / self.last_price) - 1
        
        # VaR and CVaR
        var_95 = np.percentile(final_returns, 5)
        cvar_95 = np.mean(final_returns[final_returns <= var_95])
        
        # Max drawdowns
        max_drawdowns = []
        for path in paths:
            running_max = np.maximum.accumulate(path)
            drawdowns = (path - running_max) / running_max
            max_drawdowns.append(np.min(drawdowns))
        max_drawdowns = np.array(max_drawdowns)
        
        # Win rate
        win_rate = np.mean(final_returns > 0)
        
        # Kelly fraction
        avg_win = np.mean(final_returns[final_returns > 0]) if np.any(final_returns > 0) else 0
        avg_loss = np.abs(np.mean(final_returns[final_returns < 0])) if np.any(final_returns < 0) else 1
        kelly = (win_rate / avg_loss) - ((1 - win_rate) / avg_win) if avg_win > 0 else 0
        kelly = max(0, min(kelly, 1.0))
        
        return {
            'var_95': float(var_95),
            'cvar_95': float(cvar_95),
            'max_drawdowns': max_drawdowns,
            'prob_dd_20': float(np.mean(max_drawdowns < -0.20)),
            'prob_dd_30': float(np.mean(max_drawdowns < -0.30)),
            'win_rate': float(win_rate),
            'kelly_fraction': float(kelly),
            'mean_return': float(np.mean(final_returns)),
            'median_return': float(np.median(final_returns)),
        }
    
    def compute_historical_hit_rates(self) -> Dict:
        """
        Compute actual historical first-passage hit rates.
        Uses NON-OVERLAPPING windows to avoid autocorrelation.
        """
        closes = self.data[self._price_col].values
        n = len(closes)
        horizon = self.days_ahead
        
        up_hits = 0
        down_hits = 0
        windows = 0
        
        # Non-overlapping windows
        step = horizon  # Non-overlapping
        for t in range(0, n - horizon, step):
            start_price = closes[t]
            window = closes[t:t + horizon]
            
            up_target = start_price * (self.target_up / self.last_price)
            down_target = start_price * (self.target_down / self.last_price)
            
            if np.max(window) >= up_target:
                up_hits += 1
            if np.min(window) <= down_target:
                down_hits += 1
            windows += 1
        
        return {
            'hist_up_hit': up_hits / windows if windows > 0 else 0,
            'hist_down_hit': down_hits / windows if windows > 0 else 0,
            'n_windows': windows,
        }
    
    def calibrate_targets_from_path_extrema(
        self, 
        horizon: Optional[int] = None, 
        confidence: float = 0.05
    ) -> Dict:
        """
        Calibrate targets using forward-looking max/min excursions.
        
        This is the CORRECT way to set targets per the backlog P0-B:
        - M^up_t = max_{1<=k<=H} ln(P_{t+k}/P_t)
        - M^down_t = min_{1<=k<=H} ln(P_{t+k}/P_t)
        
        Uses NON-OVERLAPPING windows to avoid autocorrelation inflation.
        
        Args:
            horizon: Forecast horizon in days (default: self.days_ahead)
            confidence: Quantile for target setting (default: 5% tails)
            
        Returns:
            Dict with calibrated target_up and target_down
        """
        horizon = horizon or self.days_ahead
        prices = self.data[self._price_col].values
        n = len(prices)
        
        max_excursions = []
        min_excursions = []
        
        # Non-overlapping windows to avoid autocorrelation
        for t in range(0, n - horizon, horizon):
            window = prices[t:t + horizon]
            start_price = prices[t]
            
            # Log excursions (path extrema)
            max_log_excursion = np.log(np.max(window) / start_price)
            min_log_excursion = np.log(np.min(window) / start_price)
            
            max_excursions.append(max_log_excursion)
            min_excursions.append(min_log_excursion)
        
        max_excursions = np.array(max_excursions)
        min_excursions = np.array(min_excursions)
        
        # Target up: (1-confidence) quantile of max excursions
        # Target down: confidence quantile of min excursions
        up_threshold = np.percentile(max_excursions, 100 * (1 - confidence))
        down_threshold = np.percentile(min_excursions, 100 * confidence)
        
        calibrated_up = self.last_price * np.exp(up_threshold)
        calibrated_down = self.last_price * np.exp(down_threshold)
        
        return {
            'target_up': float(calibrated_up),
            'target_down': float(calibrated_down),
            'up_threshold_pct': float(np.exp(up_threshold) - 1),
            'down_threshold_pct': float(np.exp(down_threshold) - 1),
            'n_windows': len(max_excursions),
            'max_excursion_mean': float(np.mean(max_excursions)),
            'min_excursion_mean': float(np.mean(min_excursions)),
        }
    
    def spawn_worker_rngs(self, n_workers: int) -> List:
        """
        Spawn independent RNGs for parallel workers.
        
        Uses SeedSequence to ensure:
        1. Each worker has independent random stream
        2. Results are reproducible given same seed
        3. No RNG collisions between workers
        
        Args:
            n_workers: Number of parallel workers
            
        Returns:
            List of numpy Generator objects
        """
        child_seeds = self._seed_seq.spawn(n_workers)
        self._worker_rngs = [np.random.default_rng(s) for s in child_seeds]
        return self._worker_rngs
    
    def simulate_sparse(self) -> Dict:
        """
        Memory-optimized simulation that only tracks hitting times.
        
        For production mode (50k+ sims), storing full paths is expensive.
        This method only stores hitting times and key statistics.
        
        Returns:
            Dict with hitting times, max drawdowns, and summary stats
        """
        n_sims = self.config.simulations
        n_days = self.days_ahead
        
        print(f"[MODE B SPARSE] Simulating {n_sims:,} paths x {n_days} days...")
        
        # Only track what we need
        tau_up = np.full(n_sims, np.inf)
        tau_down = np.full(n_sims, np.inf)
        max_drawdowns = np.zeros(n_sims)
        final_prices = np.zeros(n_sims)
        
        # Get historical data as arrays for fast indexing
        n_hist = len(self.pair_db)
        mkt_rets = self.pair_db['mkt_ret'].values
        residuals = self.pair_db['residual'].values
        
        p_new_block = 1.0 / self.config.mean_block_length
        
        for s in range(n_sims):
            price = self.last_price
            running_max = price
            block_idx = self.rng.integers(0, n_hist)
            
            for d in range(n_days):
                if self.rng.random() < p_new_block:
                    block_idx = self.rng.integers(0, n_hist)
                else:
                    block_idx = (block_idx + 1) % n_hist
                
                mkt_ret = mkt_rets[block_idx]
                eps = residuals[block_idx]
                ret = self.alpha + self.beta * mkt_ret + eps
                
                price = price * np.exp(ret)
                running_max = max(running_max, price)
                
                # Track first passage
                if tau_up[s] == np.inf and price >= self.target_up:
                    tau_up[s] = d + 1
                if tau_down[s] == np.inf and price <= self.target_down:
                    tau_down[s] = d + 1
                
                # Track drawdown
                dd = (price - running_max) / running_max
                if dd < max_drawdowns[s]:
                    max_drawdowns[s] = dd
            
            final_prices[s] = price
        
        print(f"[MODE B SPARSE] Complete. Memory: ~{(n_sims * 4 * 4) / 1024:.1f} KB")
        
        return {
            'tau_up': tau_up,
            'tau_down': tau_down,
            'max_drawdowns': max_drawdowns,
            'final_prices': final_prices,
            'prob_up': float(np.mean(tau_up < np.inf)),
            'prob_down': float(np.mean(tau_down < np.inf)),
            'mean_final_return': float(np.mean(final_prices / self.last_price) - 1),
            'prob_dd_20': float(np.mean(max_drawdowns < -0.20)),
            'prob_dd_30': float(np.mean(max_drawdowns < -0.30)),
        }
    
    def run(self, verbose: bool = True) -> Dict:
        """
        Full Mode B pipeline with diagnostics.
        
        Returns comprehensive results dict.
        """
        if self.data is None:
            self.ingest_data()
        
        # Run simulation
        paths = self.simulate()
        
        # Compute all outputs
        diagnostics = self.compute_diagnostics(paths)
        fpt = self.analyze_first_passage(paths)
        risk = self.compute_risk_metrics(paths)
        hist_rates = self.compute_historical_hit_rates()
        
        # Check gatekeeper pass status
        gatekeepers = [d for d in diagnostics if d.is_gatekeeper]
        all_gates_passed = all(d.passed for d in gatekeepers)
        
        if verbose:
            print("\n" + "="*60)
            print("MODE B DIAGNOSTICS (Stationary Bootstrap)")
            print("="*60)
            for d in diagnostics:
                print(d)
            print("-"*60)
            print(f"GATEKEEPER STATUS: {'âœ… ALL PASSED' if all_gates_passed else 'âŒ SOME FAILED'}")
            print("="*60)
            
            print(f"\nðŸ“Š First-Passage Analysis:")
            print(f"   P(Up First): {fpt['prob_up_first']:.1%}")
            print(f"   P(Down First): {fpt['prob_down_first']:.1%}")
            print(f"   P(Neither): {fpt['prob_neither']:.1%}")
            print(f"   Historical Up Hit: {hist_rates['hist_up_hit']:.1%}")
            print(f"   Historical Down Hit: {hist_rates['hist_down_hit']:.1%}")
            
            print(f"\nðŸ“‰ Risk Metrics:")
            print(f"   VaR(95): {risk['var_95']*100:+.1f}%")
            print(f"   CVaR(95): {risk['cvar_95']*100:+.1f}%")
            print(f"   P(MaxDD > 20%): {risk['prob_dd_20']:.1%}")
            print(f"   Kelly: {risk['kelly_fraction']:.0%}")
        
        return {
            'paths': paths,
            'diagnostics': diagnostics,
            'first_passage': fpt,
            'risk': risk,
            'historical_validation': hist_rates,
            'all_gates_passed': all_gates_passed,
            'config': {
                'mode': self.config.mode,
                'simulations': self.config.simulations,
                'mean_block_length': self.config.mean_block_length,
            },
            'metadata': {
                'ticker': self.ticker,
                'market_ticker': self.market_ticker,
                'last_price': self.last_price,
                'beta': self.beta,
                'alpha_ann': self.alpha * 252,
                'realized_vol': self.realized_vol,
                'target_up': self.target_up,
                'target_down': self.target_down,
                'timestamp': datetime.now().isoformat(),
            }
        }


# =============================================================================
# FROZEN EVALUATION SUITE (Anti-Overfit Guardrail)
# =============================================================================
# Per P0.5.3: Define a frozen, minimal evaluation suite used for every change.
# No feature "ships" unless it improves or does not degrade scores on this suite.

FROZEN_ASSETS = ["SPY", "QQQ", "PLTR"]  # 1 broad ETF, 1 tech beta, 1 idiosyncratic
FROZEN_HORIZONS = [20, 126]  # Short-term and 6-month
FROZEN_THRESHOLDS = [0.10, 0.30]  # 10% and 30% targets


def run_frozen_evaluation_suite(
    config: Optional[BootstrapConfig] = None,
    verbose: bool = True
) -> Dict:
    """
    Run Mode B on the frozen evaluation suite.
    
    This is the ANTI-OVERFIT guardrail from the backlog (P0.5.3):
    - Tests against multiple assets to ensure generalization
    - Uses frozen horizons and thresholds
    - Reports aggregate pass/fail across all gatekeepers
    
    Rule: Improvements must generalize across â‰¥ 3 assets.
    
    Args:
        config: Bootstrap configuration (default: fast mode)
        verbose: Print detailed results
        
    Returns:
        Dict with pass/fail status and per-asset results
    """
    config = config or BootstrapConfig(mode="debug", seed=42)
    
    results = {}
    all_gatekeepers_passed = []
    
    if verbose:
        print("\n" + "="*70)
        print("FROZEN EVALUATION SUITE - Mode B Generalization Test")
        print("="*70)
        print(f"Assets: {FROZEN_ASSETS}")
        print(f"Horizons: {FROZEN_HORIZONS}")
        print(f"Thresholds: {FROZEN_THRESHOLDS}")
        print("-"*70)
    
    for ticker in FROZEN_ASSETS:
        for horizon in FROZEN_HORIZONS:
            key = f"{ticker}_{horizon}d"
            
            if verbose:
                print(f"\nâ–¶ Testing {ticker} @ {horizon} days...")
            
            try:
                engine = StationaryBootstrap(
                    ticker=ticker,
                    market_ticker="SPY" if ticker != "SPY" else "QQQ",
                    days_ahead=horizon,
                    config=config,
                )
                engine.ingest_data()
                
                # Auto-calibrate targets from path extrema
                calibration = engine.calibrate_targets_from_path_extrema(
                    horizon=horizon, 
                    confidence=0.05
                )
                engine.target_up = calibration['target_up']
                engine.target_down = calibration['target_down']
                
                # Run simulation
                paths = engine.simulate()
                diagnostics = engine.compute_diagnostics(paths)
                
                # Check gatekeepers
                gatekeepers = [d for d in diagnostics if d.is_gatekeeper]
                passed = all(d.passed for d in gatekeepers)
                all_gatekeepers_passed.append(passed)
                
                # ACF score (key metric)
                acf_scores = [d.sim_value for d in diagnostics if "ACF" in d.metric]
                
                results[key] = {
                    'passed': passed,
                    'gatekeeper_results': [
                        {'metric': d.metric, 'passed': d.passed, 'delta': abs(d.sim_value - d.hist_value)}
                        for d in gatekeepers
                    ],
                    'beta': engine.beta,
                    'realized_vol': engine.realized_vol,
                    'calibrated_targets': calibration,
                }
                
                if verbose:
                    status = "âœ… PASS" if passed else "âŒ FAIL"
                    print(f"   {status} | Beta={engine.beta:.2f} | Vol={engine.realized_vol:.1%}")
                    for d in gatekeepers:
                        g_status = "âœ…" if d.passed else "âŒ"
                        print(f"      {g_status} {d.metric}: Î”={abs(d.sim_value - d.hist_value):.4f}")
                        
            except Exception as e:
                results[key] = {'passed': False, 'error': str(e)}
                all_gatekeepers_passed.append(False)
                if verbose:
                    print(f"   âŒ ERROR: {e}")
    
    # Aggregate results
    n_passed = sum(all_gatekeepers_passed)
    n_total = len(all_gatekeepers_passed)
    generalization_score = n_passed / n_total if n_total > 0 else 0
    
    # Rule: Must pass on at least 3 assets
    suite_passed = n_passed >= 3
    
    if verbose:
        print("\n" + "="*70)
        print(f"SUITE RESULT: {'âœ… PASS' if suite_passed else 'âŒ FAIL'}")
        print(f"Generalization Score: {n_passed}/{n_total} ({generalization_score:.0%})")
        print(f"Requirement: â‰¥3 tests must pass")
        print("="*70)
    
    return {
        'suite_passed': suite_passed,
        'generalization_score': generalization_score,
        'passed_count': n_passed,
        'total_count': n_total,
        'per_asset_results': results,
    }


# =============================================================================
# CLI for standalone testing
# =============================================================================
if __name__ == "__main__":
    import sys
    
    ticker = sys.argv[1] if len(sys.argv) > 1 else "PLTR"
    
    print(f"\n{'='*60}")
    print(f"MODE B: STATIONARY BOOTSTRAP - {ticker}")
    print(f"{'='*60}\n")
    
    config = BootstrapConfig(mode="fast", seed=42)
    engine = StationaryBootstrap(
        ticker=ticker,
        market_ticker="QQQ",
        days_ahead=126,
        config=config,
    )
    
    results = engine.run(verbose=True)
    
    print(f"\nâœ… DONE. Paths shape: {results['paths'].shape}")
